{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten\n",
    "from keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained embeddings\n",
    "# The first 15000\n",
    "embedding_file = open('./GoogleNews-vectors-negative300_lite.txt')\n",
    "embedding_file.readline()\n",
    "embedding_indices = dict()\n",
    "for line in embedding_file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype=np.float32)\n",
    "    embedding_indices[word] = coefs\n",
    "embedding_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for training\n",
    "# - model generation\n",
    "# - metrics scraper\n",
    "def get_model(e):\n",
    "    model = Sequential()\n",
    "    model.add(e)\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['acc'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "class EpochHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accs.append(logs.get('acc'))\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables to access dataset\n",
    "dataset_base = \"lexical_entailment\"\n",
    "datasets = [\"baroni2012\", \"bless2011\", \"kotlerman2010\", \"levy2014\", \"turney2014\"]\n",
    "chosen_dataset = 0\n",
    "test = \"data_lex_test.tsv\"\n",
    "train = \"data_lex_train.tsv\"\n",
    "val = \"data_lex_val.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "glob = pd.read_csv(\n",
    "    os.path.join(dataset_base, datasets[chosen_dataset], 'data.tsv')\n",
    "    , sep='\\t', header=None)\n",
    "\n",
    "glob_text_np = np.array(glob.loc[:, 0:1]).flatten()\n",
    "glob_text_uniq = list(set(glob_text_np))\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(glob_text_uniq)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4834 words found out of 5623 in the 149999 item long embedding corpus.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the embedder matrix for the pretrained embedder\n",
    "num_words_found = 0\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_indices.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        num_words_found+=1\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(\"%d words found out of %d in the %d item long embedding corpus.\"%\n",
    "      (num_words_found, vocab_size, len(embedding_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google v2w:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 2, 300)            1686900   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2, 64)             19264     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,706,293\n",
      "Trainable params: 19,393\n",
      "Non-trainable params: 1,686,900\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Vanila embedding:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 2, 300)            1686900   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2, 64)             19264     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,706,293\n",
      "Trainable params: 1,706,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Creating the models\n",
    "print(\"Google v2w:\")\n",
    "embedding1 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=2, trainable=False)\n",
    "model1 = get_model(embedding1)\n",
    "print()\n",
    "print(\"Vanila embedding:\")\n",
    "embedding2 = Embedding(vocab_size, 300, input_length=2, trainable=True)\n",
    "model2 = get_model(embedding2)\n",
    "\n",
    "models = [model1, model2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "train_glob = pd.read_csv(\n",
    "    os.path.join(dataset_base, datasets[chosen_dataset], train)\n",
    "    , sep='\\t', header=None)\n",
    "\n",
    "val_glob = pd.read_csv(\n",
    "    os.path.join(dataset_base, datasets[chosen_dataset], val)\n",
    "    , sep='\\t', header=None)\n",
    "\n",
    "test_glob = pd.read_csv(\n",
    "    os.path.join(dataset_base, datasets[chosen_dataset], test)\n",
    "    , sep='\\t', header=None)\n",
    "\n",
    "train_X = np.array(train_glob.loc[:, 0:1])\n",
    "train_X = np.array([[tokenizer.texts_to_sequences([txt])[0][0] for txt in row ]for row in train_X]).squeeze()\n",
    "train_Y = np.array(train_glob.loc[:, 2])\n",
    "train_Y = train_Y * 1\n",
    "\n",
    "val_X = np.array(val_glob.loc[:, 0:1])\n",
    "val_X = np.array([[tokenizer.texts_to_sequences([txt])[0][0] for txt in row ]for row in val_X]).squeeze()\n",
    "val_Y = np.array(val_glob.loc[:, 2])\n",
    "val_Y = val_Y * 1\n",
    "\n",
    "test_X = np.array(test_glob.loc[:, 0:1])\n",
    "test_X = np.array([[tokenizer.texts_to_sequences([txt])[0][0] for txt in row ]for row in test_X]).squeeze()\n",
    "test_Y = np.array(test_glob.loc[:, 2])\n",
    "test_Y = test_Y * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Training model #0 ##\n"
     ]
    }
   ],
   "source": [
    "# Training the models\n",
    "accs = list()\n",
    "losses = list()\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    history = EpochHistory()\n",
    "    print(\"## Training model #%d ##\" % i)\n",
    "    model.fit(train_X, train_Y, epochs=100, validation_data=(val_X, val_Y), callbacks=[history], verbose=0)\n",
    "    accs.append(history.accs)\n",
    "    losses.append(history.losses)\n",
    "    print(\"## Testing model #%d ##\" % i)\n",
    "    t_loss, t_acc = model.evaluate(test_X, test_Y, verbose=0)\n",
    "    print(\"Test Results: loss=%f, acc=%f\" % (t_loss, t_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_array = accs # losses\n",
    "show_array = np.array(show_array).T\n",
    "labels = ['Google w2v', 'Vanilla Embedding']\n",
    "\n",
    "plt.xlabel(labels[0])\n",
    "plt.ylabel(labels[1])\n",
    "plt.plot([show_array.min(), show_array.max()], [show_array.min(), show_array.max()])\n",
    "plt.plot(show_array[:, 0], show_array[:, 1], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_array = losses\n",
    "show_array = np.array(show_array).T\n",
    "labels = ['Google w2v', 'Vanilla Embedding']\n",
    "plt.xlabel(labels[0])\n",
    "plt.ylabel(labels[1])\n",
    "plt.plot([show_array.min(), show_array.max()], [show_array.min(), show_array.max()])\n",
    "plt.plot(show_array[:, 0], show_array[:, 1], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 317,   83],\n",
       "       [ 912, 1108],\n",
       "       [ 665, 1148],\n",
       "       ...,\n",
       "       [ 271, 1028],\n",
       "       [ 305,  370],\n",
       "       [1313,  388]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
